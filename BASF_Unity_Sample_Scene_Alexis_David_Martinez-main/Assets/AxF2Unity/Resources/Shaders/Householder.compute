#pragma kernel CSHouseholderComputeAlphaAndR
#pragma kernel CSHouseholderMatMul

#ifndef STORAGE_HLSL
#define STORAGE_HLSL

#define DECLARE_MATRIX_INTERNAL(buffer, scalar, mat) \
    buffer<scalar> mat; \
    int mat##Start; \
    int mat##RowStride; \
    int mat##RowCount; \
    int mat##ColStride; \
    int mat##ColCount;

#define DECLARE_MATRIX(mat) DECLARE_MATRIX_INTERNAL(StructuredBuffer, float, mat)
#define DECLARE_MATRIX_RW(mat) DECLARE_MATRIX_INTERNAL(RWStructuredBuffer, float, mat)

#define MATRIX_AT(mat, r, c) mat[mat##Start + r * mat##RowStride + c * mat##ColStride]
#define MATRIX_ROW_COUNT(mat) mat##RowCount
#define MATRIX_COL_COUNT(mat) mat##ColCount

#define DECLARE_VECTOR_INTERNAL(buffer, scalar, vec) \
    RWStructuredBuffer<float> vec; \
    int vec##Start; \
    int vec##Stride; \
    int vec##Count;

#define DECLARE_VECTOR(vec) DECLARE_VECTOR_INTERNAL(StructuredBuffer, float, vec)
#define DECLARE_VECTOR_RW(vec) DECLARE_VECTOR_INTERNAL(RWStructuredBuffer, float, vec)

#define VECTOR_AT(vec, i) vec[vec##Start + i * vec##Stride]
#define VECTOR_SIZE(vec) vec##Count

#endif // STORAGE_HLSL
#ifndef GROUPREDUCE_HLSL
#define GROUPREDUCE_HLSL

groupshared float sharedData[32];

float GroupReduce(float value, int tid)
{
    sharedData[tid] = value;

    // Final reduce in shared memory
    // barrier();
    AllMemoryBarrierWithGroupSync();

    if (tid < 16)
        sharedData[tid] += sharedData[tid + 16];
    AllMemoryBarrierWithGroupSync();

    if (tid < 8)
        sharedData[tid] += sharedData[tid + 8];
    AllMemoryBarrierWithGroupSync();

    if (tid < 4)
        sharedData[tid] += sharedData[tid + 4];
    AllMemoryBarrierWithGroupSync();

    if (tid < 2)
        sharedData[tid] += sharedData[tid + 2];
    AllMemoryBarrierWithGroupSync();

    if (tid < 1)
        sharedData[tid] += sharedData[tid + 1];
    AllMemoryBarrierWithGroupSync();

    return sharedData[0];
}

#endif // GROUPREDUCE_HLSL

DECLARE_MATRIX(_InputMatrix);
DECLARE_MATRIX_RW(_OutputMatrix);

DECLARE_VECTOR_RW(_Alpha);
DECLARE_VECTOR_RW(_R);



float NormOfFirstInputRow(int threadIndex, int threadCount)
{
    int rowCount = MATRIX_ROW_COUNT(_InputMatrix);

    float squaredLength = 0;
    for (int i = threadIndex; i < rowCount; i += threadCount)
    {
        float value = MATRIX_AT(_InputMatrix, i, 0);
        squaredLength += value*value;
    }
    squaredLength = GroupReduce(squaredLength, threadIndex);

    return sqrt(squaredLength);
}


[numthreads(32,1,1)]
void CSHouseholderComputeAlphaAndR(uint3 groupThreadID: SV_GroupThreadID)
{
    int threadIndex = groupThreadID.x;
    int threadCount = 32;

    float y0 = MATRIX_AT(_InputMatrix, 0, 0);
    int sign = y0 < 0 ? -1 : 1;

    float a = sign * NormOfFirstInputRow(threadIndex, threadCount);
    if (threadIndex == 0)
        VECTOR_AT(_Alpha, 0) = -a;

    float scale = 1 / sqrt((y0 + a) * a);

    int rowCount = MATRIX_ROW_COUNT(_InputMatrix);
    for (int i = threadIndex; i < rowCount; i += threadCount)
    {
        float value = i == 0 ? (y0 + a) : MATRIX_AT(_InputMatrix, i, 0);
        VECTOR_AT(_R, i) = value * scale;
    }
}



[numthreads(1,32,1)]
void CSHouseholderMatMul(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    // H = np.eye(y.shape[0]) - r[:, None] * r[None, :]
    // A[i:, i:] = H @ A[i:, i:]
    // Q[:, i:] = Q[:, i:] @ H
    // In our case, _InputMatrix is already sliced
    // Transposition of Q is implicit in row/col strides

    int colIndex = dispatchThreadID.x;
    int rowIndex = dispatchThreadID.y;

    int rowCount = MATRIX_ROW_COUNT(_OutputMatrix);
    int colCount = MATRIX_COL_COUNT(_OutputMatrix);

    if (colIndex >= colCount || rowIndex >= rowCount)
        return;


    int j = rowIndex;
    float R_j = VECTOR_AT(_R, j);

    float result = 0;
    for (int i = 0; i < rowCount; ++i)
    {
        float A_ij = MATRIX_AT(_InputMatrix, i, colIndex);
        float R_i = VECTOR_AT(_R, i);

        float H_ij = (i == j ? 1 : 0) - R_i * R_j;

        result += H_ij * A_ij;
    }

    MATRIX_AT(_OutputMatrix, rowIndex, colIndex) = result;
}
